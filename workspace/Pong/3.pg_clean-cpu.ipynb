{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resize = transforms.Compose([transforms.ToPILImage(),transforms.Scale(size=40),transforms.ToTensor()])\n",
    "\n",
    "def preprocess(obs):\n",
    "    tensor = torch.from_numpy(obs.transpose((2, 0, 1)))\n",
    "    resized = resize(tensor)\n",
    "    return resized.unsqueeze(0)\n",
    "\n",
    "def clip_grads(net, low=-10, high=10):\n",
    "    \"\"\"Gradient clipping to the range [low, high].\"\"\"\n",
    "    parameters = [param for param in net.parameters()\n",
    "                  if param.grad is not None]\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(low, high)\n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    def to_var(x, requires_grad=False, gpu=None):\n",
    "        x = x.cuda(gpu)\n",
    "        return Variable(x, requires_grad=requires_grad)\n",
    "else:\n",
    "    def to_var(x, requires_grad=False, vgpu=None):\n",
    "        return Variable(x, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, action_n):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(3,32,kernel_size=5),nn.ReLU(),nn.MaxPool2d(kernel_size=2),\n",
    "                                  nn.Conv2d(32,128,kernel_size=5),nn.ReLU(),nn.MaxPool2d(kernel_size=2),\n",
    "                                  nn.Conv2d(128,32,kernel_size=5),nn.ReLU(),nn.AdaptiveMaxPool2d(output_size=1))\n",
    "        self.fc = nn.Linear(32, action_n)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)\n",
    "        logit = self.fc(feat.view(feat.size(0),-1))\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    \n",
    "    def __init__(self, model, gamma=0.99, eps=1.e-6, running_gamma=0.99, running_start=0,\n",
    "#                 episode2thresh=lambda i: 0.05+0.9*np.exp(-1. * i / 100) if i>150 else 0): # eploration will start after 150 episodes\n",
    "                 episode2thresh=lambda i: 0): # without exploration\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.total_rewards = []\n",
    "        self.running_reward = running_start\n",
    "        self.running_gamma = running_gamma\n",
    "        self.episode2thresh = episode2thresh\n",
    "        \n",
    "    @property\n",
    "    def episodes(self):\n",
    "        return len(self.total_rewards)\n",
    "        \n",
    "    def select_action(self,obs):\n",
    "        self.model.train()\n",
    "        thresh=self.episode2thresh(self.episodes)\n",
    "        action, log_prob = select_action(obs, self.model, thresh=thresh)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def get_loss_and_clear(self):\n",
    "        total_reward = sum(self.rewards)\n",
    "        self.total_rewards.append(total_reward)\n",
    "        self.running_reward = self.running_gamma*self.running_reward+(1-self.running_gamma)*total_reward\n",
    "        policy_loss = get_policy_loss(self.log_probs, self.rewards, self.gamma, self.eps)\n",
    "        del self.log_probs[:]\n",
    "        del self.rewards[:]\n",
    "        return policy_loss\n",
    "    \n",
    "    def take_action(self, action, env, render=False):\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        self.rewards.append(reward)\n",
    "        if render:\n",
    "            env.render()\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def greedy_policy(self, obs):\n",
    "        self.model.eval()\n",
    "        state = to_var(preprocess(obs))\n",
    "        prob = self.model(state)\n",
    "        _, action = prob.max(dim=1)\n",
    "        return action.data[0]\n",
    "\n",
    "def select_action(obs, model, thresh=0):\n",
    "    state = to_var(preprocess(obs))\n",
    "    logits = model(state)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    m = Categorical(probs)\n",
    "    if np.random.random()>thresh:\n",
    "#         print(probs)\n",
    "        try:\n",
    "            action = m.sample()\n",
    "        except:\n",
    "            print(probs,m)\n",
    "            raise\n",
    "    else:\n",
    "        action_space = probs.size(1)\n",
    "        action = to_var(torch.from_numpy(np.random.randint(action_space,size=1)))\n",
    "    return action.data[0],m.log_prob(action)\n",
    "    \n",
    "def get_normalized_rewards(rewards, gamma, eps):\n",
    "    acc = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        acc.append(R)\n",
    "    ret = to_var(torch.Tensor(acc[::-1]),requires_grad=False)\n",
    "    ret = (ret - ret.mean()) / (ret.std()+eps)\n",
    "#     print(ret)\n",
    "    return ret\n",
    "\n",
    "def get_policy_loss(log_probs,rewards, gamma,eps):\n",
    "    log_probs_v = torch.cat(log_probs)\n",
    "    rewards_v = get_normalized_rewards(rewards, gamma, eps)\n",
    "    return -log_probs_v.dot(rewards_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net(env.action_space.n)\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1.e-4,weight_decay=0.001)\n",
    "trainer = PolicyGradient(model=net,running_start=-21)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "writer_path = list(writer.all_writers.keys())[0]\n",
    "weight_join = lambda p: os.path.join(writer_path, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:07<210:08:04,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -20.0 -20.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 101/100000 [09:58<159:51:25,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -21.0 -20.58992511819085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 200/100000 [19:54<164:10:13,  5.92s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ceaddc6a1d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#     print(policy_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mclip_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiancheng/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiancheng/anaconda3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 83\u001b[0;31m         variables, grad_variables, retain_graph, create_graph)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in trange(100000):\n",
    "    last_frame = frame = env.reset()\n",
    "    obs = frame-last_frame\n",
    "    total_reward = 0\n",
    "    for step in range(100000): # not exceed 10000 steps\n",
    "        action = trainer.select_action(obs)\n",
    "        frame, reward, done, _ = trainer.take_action(action, env, render=False)\n",
    "        obs = frame-last_frame\n",
    "        last_frame=frame\n",
    "        total_reward+=reward\n",
    "        if done:\n",
    "             break\n",
    "    if step==100000:\n",
    "        print(\"not enough!!!!!!!!!!!!!!!\")\n",
    "    policy_loss = trainer.get_loss_and_clear()\n",
    "    writer.add_scalar(\"loss\",policy_loss.data[0],episode)\n",
    "    writer.add_scalar(\"reward\",total_reward,episode)\n",
    "#     print(policy_loss)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    clip_grads(trainer.model,-5,5)\n",
    "    optimizer.step()\n",
    "    running_reward = trainer.running_reward\n",
    "    if episode%100==0:\n",
    "        print(episode, total_reward,running_reward)\n",
    "        torch.save(net.state_dict(), weight_join(\"episode%s.pth\"%episode))\n",
    "    if running_reward>1:\n",
    "        break\n",
    "print(\"Finished: %s@%s\" %(trainer.running_reward,episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), weight_join(\"final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(trainer.total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
