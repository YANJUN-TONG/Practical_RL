{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downsample = 4\n",
    "\n",
    "def preprocess(frame):\n",
    "    '''from karpathy.'''\n",
    "    I = frame\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::downsample,::downsample,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    tensor = torch.from_numpy(I).float()\n",
    "    return tensor.unsqueeze(0).unsqueeze(0) #BCHW\n",
    "\n",
    "def clip_grads(net, low=-10, high=10):\n",
    "    \"\"\"Gradient clipping to the range [low, high].\"\"\"\n",
    "    parameters = [param for param in net.parameters()\n",
    "                  if param.grad is not None]\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(low, high)\n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    def to_var(x, requires_grad=False, gpu=None):\n",
    "        x = x.cuda(gpu)\n",
    "        return Variable(x, requires_grad=requires_grad)\n",
    "else:\n",
    "    def to_var(x, requires_grad=False, vgpu=None):\n",
    "        return Variable(x, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, action_n):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(1,32,kernel_size=5),nn.ReLU(),nn.MaxPool2d(kernel_size=2),\n",
    "                                  nn.Conv2d(32,128,kernel_size=5),nn.ReLU(),nn.MaxPool2d(kernel_size=2),\n",
    "                                  nn.Conv2d(128,32,kernel_size=5),nn.ReLU(),nn.AdaptiveMaxPool2d(output_size=1))\n",
    "        self.fc = nn.Linear(32, action_n)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)\n",
    "        logit = self.fc(feat.view(feat.size(0),-1))\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    \n",
    "    def __init__(self, model, gamma=0.99, eps=1.e-6, running_gamma=0.99, running_start=0,\n",
    "#                 episode2thresh=lambda i: 0.05+0.9*np.exp(-1. * i / 100) if i>150 else 0): # eploration will start after 150 episodes\n",
    "                 episode2thresh=lambda i: 0): # without exploration\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.total_rewards = []\n",
    "        self.running_reward = running_start\n",
    "        self.running_gamma = running_gamma\n",
    "        self.episode2thresh = episode2thresh\n",
    "        \n",
    "    @property\n",
    "    def episodes(self):\n",
    "        return len(self.total_rewards)\n",
    "        \n",
    "    def select_action(self,obs):\n",
    "        self.model.train()\n",
    "        thresh=self.episode2thresh(self.episodes)\n",
    "        action, log_prob = select_action(obs, self.model, thresh=thresh)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def get_loss_and_clear(self):\n",
    "        total_reward = sum(self.rewards)\n",
    "        self.total_rewards.append(total_reward)\n",
    "        self.running_reward = self.running_gamma*self.running_reward+(1-self.running_gamma)*total_reward\n",
    "        policy_loss = get_policy_loss(self.log_probs, self.rewards, self.gamma, self.eps)\n",
    "        del self.log_probs[:]\n",
    "        del self.rewards[:]\n",
    "        return policy_loss\n",
    "    \n",
    "    def take_action(self, action, env, render=False):\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        self.rewards.append(reward)\n",
    "        if render:\n",
    "            env.render()\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def greedy_policy(self, obs):\n",
    "        self.model.eval()\n",
    "        state = to_var(obs)\n",
    "        prob = self.model(state)\n",
    "        _, action = prob.max(dim=1)\n",
    "        return action.data[0]\n",
    "\n",
    "def select_action(obs, model, thresh=0):\n",
    "    state = to_var(obs)\n",
    "    logits = model(state)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    m = Categorical(probs)\n",
    "    if np.random.random()>thresh:\n",
    "#         print(probs)\n",
    "        try:\n",
    "            action = m.sample()\n",
    "        except:\n",
    "            print(probs,m)\n",
    "            raise\n",
    "    else:\n",
    "        action_space = probs.size(1)\n",
    "        action = to_var(torch.from_numpy(np.random.randint(action_space,size=1)))\n",
    "    return action.data[0],m.log_prob(action)\n",
    "    \n",
    "def get_normalized_rewards(rewards, gamma, eps):\n",
    "    acc = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        acc.append(R)\n",
    "    ret = to_var(torch.Tensor(acc[::-1]),requires_grad=False)\n",
    "    ret = (ret - ret.mean()) / (ret.std()+eps)\n",
    "#     print(ret)\n",
    "    return ret\n",
    "\n",
    "def get_policy_loss(log_probs,rewards, gamma,eps):\n",
    "    log_probs_v = torch.cat(log_probs)\n",
    "    rewards_v = get_normalized_rewards(rewards, gamma, eps)\n",
    "    return -log_probs_v.dot(rewards_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net(env.action_space.n)\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr=3.e-4,weight_decay=0.001)\n",
    "trainer = PolicyGradient(model=net,running_start=-21)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "writer_path = list(writer.all_writers.keys())[0]\n",
    "weight_join = lambda p: os.path.join(writer_path, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:05<154:27:08,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -20.0 -20.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 38/100000 [02:49<131:31:32,  4.74s/it]"
     ]
    }
   ],
   "source": [
    "for episode in trange(100000):\n",
    "    frame = env.reset()\n",
    "    last_obs = preprocess(frame)\n",
    "    curr_obs = preprocess(frame)\n",
    "    total_reward = 0\n",
    "    for step in range(100000): # not exceed 10000 steps\n",
    "        action = trainer.select_action(obs=curr_obs-last_obs)\n",
    "        frame, reward, done, _ = trainer.take_action(action, env, render=False)\n",
    "        last_obs = curr_obs\n",
    "        curr_obs = preprocess(frame)\n",
    "        total_reward+=reward\n",
    "        if done:\n",
    "             break\n",
    "    if step==100000:\n",
    "        print(\"not enough!!!!!!!!!!!!!!!\")\n",
    "    policy_loss = trainer.get_loss_and_clear()\n",
    "    writer.add_scalar(\"loss\",policy_loss.data[0],episode)\n",
    "    writer.add_scalar(\"reward\",total_reward,episode)\n",
    "#     print(policy_loss)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    clip_grads(trainer.model,-5,5)\n",
    "    optimizer.step()\n",
    "    running_reward = trainer.running_reward\n",
    "    if episode%100==0:\n",
    "        print(episode, total_reward,running_reward)\n",
    "        torch.save(net.state_dict(), weight_join(\"episode%s.pth\"%episode))\n",
    "    if running_reward>1:\n",
    "        break\n",
    "print(\"Finished: %s@%s\" %(trainer.running_reward,episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), weight_join(\"final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(trainer.total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
