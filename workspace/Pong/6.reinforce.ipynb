{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import itertools\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downsample = 2\n",
    "output_size = 160//downsample\n",
    "\n",
    "def preprocess(frame):\n",
    "    '''from karpathy.'''\n",
    "    I = frame\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::downsample,::downsample,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    tensor = torch.from_numpy(I).float()\n",
    "    return tensor.unsqueeze(0).unsqueeze(0) #BCHW\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    def to_var(x, requires_grad=False, gpu=None):\n",
    "        x = x.cuda(gpu)\n",
    "        return Variable(x, requires_grad=requires_grad)\n",
    "else:\n",
    "    def to_var(x, requires_grad=False, vgpu=None):\n",
    "        return Variable(x, requires_grad=requires_grad)\n",
    "\n",
    "\n",
    "def clip_grads(net, low=-10, high=10):\n",
    "    \"\"\"Gradient clipping to the range [low, high].\"\"\"\n",
    "    for p in net.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.data.clamp_(low, high)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        nn.init.constant(m.bias.data,0)\n",
    "        print(\"Initialized\", m)\n",
    "        \n",
    "def total_weights(net):\n",
    "    '''Count total weights size.'''\n",
    "    ret = 0\n",
    "    for p in net.parameters():\n",
    "        ret+=p.data.cpu().numpy().size\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    '''Implement REINFORCE algorithm.'''\n",
    "    \n",
    "    def __init__(self, model, gamma=0.99, learning_rate=1.e-3, batch_size=10):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "        self.optimizer.zero_grad() # need or not?\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        self.history = []\n",
    "        \n",
    "    @property\n",
    "    def episode(self):\n",
    "        return len(self.history)\n",
    "        \n",
    "    def select_action(self, obs):\n",
    "        self.model.train()\n",
    "        state = to_var(obs)\n",
    "        logits = self.model(state)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        return action, log_prob\n",
    "    \n",
    "    def keep_for_grad(self, log_prob, reward):\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def accumulate_policy_grad(self):\n",
    "        policy_loss = get_policy_loss(self.log_probs, self.rewards, self.gamma)\n",
    "        \n",
    "        self.history.append([sum(self.rewards), # total_reward\n",
    "                             len(self.rewards), # n_round\n",
    "                             policy_loss.data[0]]) # train_loss\n",
    "        \n",
    "        policy_loss.backward()\n",
    "        del self.log_probs[:]\n",
    "        del self.rewards[:]\n",
    "        \n",
    "    def train(self):\n",
    "        clip_grads(self.model,-10,10)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        self.accumulate_policy_grad()\n",
    "        episode = self.episode\n",
    "        if episode>0 and episode%self.batch_size==0:\n",
    "            self.train()\n",
    "    \n",
    "    def play(self, obs):\n",
    "        self.model.eval()\n",
    "        state = to_var(obs)\n",
    "        prob = self.model(state)\n",
    "        _, action = prob.max(dim=1)\n",
    "        return action.data[0]\n",
    "\n",
    "def get_discounted_rewards(rewards, gamma):\n",
    "    acc = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        acc.append(R)\n",
    "    ret = np.array(acc[::-1])\n",
    "    return ret\n",
    "\n",
    "def get_normalized_rewards(rewards, gamma):\n",
    "    ret = get_discounted_rewards(rewards, gamma)\n",
    "    return (ret-ret.mean()) / (ret.std()+np.finfo(np.float32).eps)\n",
    "\n",
    "def get_policy_loss(log_probs,rewards, gamma):\n",
    "    ret = 0\n",
    "    normalized_rewards = get_normalized_rewards(rewards, gamma)\n",
    "    for log_prob, reward in zip(log_probs, normalized_rewards):\n",
    "        ret -= log_prob*reward # it's less memory consuming than dot product\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnhancedWriter(SummaryWriter):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.logdir = list(self.all_writers.keys())[0]\n",
    "        \n",
    "    def in_logdir(self, path):\n",
    "        return os.path.join(self.logdir, path)\n",
    "        \n",
    "    def save(self, model, path):\n",
    "        torch.save(model.state_dict(), self.in_logdir(path))\n",
    "        \n",
    "    def export_logs(self, filename='training.json'):\n",
    "        self.export_scalars_to_json(self.in_logdir(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''very similar to Nature DQN.'''\n",
    "    def __init__(self, action_n, input_shape=(1,80,80)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(input_shape[0],32,kernel_size=8, stride=4),nn.ReLU(),\n",
    "                                  nn.Conv2d(32,64,kernel_size=4, stride=2),nn.ReLU(),\n",
    "                                  nn.Conv2d(64,64,kernel_size=3, stride=1),nn.ReLU(),)\n",
    "        flatten_size = self._get_flatten_size(input_shape)\n",
    "        self.fc = nn.Sequential(nn.Linear(flatten_size, 512),nn.ReLU(),\n",
    "                               nn.Linear(512, action_n))\n",
    "        self.apply(weights_init)\n",
    "        print(\"Network size:\", total_weights(self))\n",
    "    \n",
    "    def _get_flatten_size(self, shape):\n",
    "        x = Variable(torch.rand(1, *shape))\n",
    "        output_feat = self.conv(x)\n",
    "        n_size = output_feat.view(-1).size(0)\n",
    "        return n_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)\n",
    "        logit = self.fc(feat.view(feat.size(0),-1))\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-09 19:14:37,917] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "Initialized Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "Initialized Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "Initialized Linear(in_features=2304, out_features=512, bias=True)\n",
      "Initialized Linear(in_features=512, out_features=6, bias=True)\n",
      "Network size: 1255078\n",
      "Net(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "net = Net(env.action_space.n, input_shape=(1,output_size,output_size))\n",
    "print(net)\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "\n",
    "agent = REINFORCE(model=net, gamma=0.99, learning_rate=1.e-3, batch_size=10)\n",
    "writer = EnhancedWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# net.load_state_dict(torch.load('runs/Jan08_15-07-08_amax/episode9100.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:05<144:01:54,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New record: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/100000 [00:30<82:56:20,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New record: -19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/100000 [01:13<86:29:33,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New record: -18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 96/100000 [04:51<87:26:29,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New record: -17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/100000 [05:03<82:37:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 -20.0 -20.2976141739793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 200/100000 [10:25<93:44:16,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 -21.0 -20.92431060192001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 245/100000 [13:02<102:44:12,  3.71s/it]"
     ]
    }
   ],
   "source": [
    "running_reward = best_reward = -21\n",
    "\n",
    "for episode in trange(100000):\n",
    "    frame = env.reset()\n",
    "    last_obs = preprocess(frame)\n",
    "    curr_obs = preprocess(frame)\n",
    "    for step in itertools.count(start=1, step=1):\n",
    "        action, log_prob = agent.select_action(obs=curr_obs-last_obs)\n",
    "        frame, reward, done, _ = env.step(action)\n",
    "        agent.keep_for_grad(log_prob, reward)\n",
    "        last_obs = curr_obs\n",
    "        curr_obs = preprocess(frame)\n",
    "        if step>=50000: # don't exceed\n",
    "            print(\"Seems much but not enough\")\n",
    "            break\n",
    "        if done:\n",
    "             break\n",
    "    agent.step() \n",
    "    \n",
    "    total_reward, n_round, train_loss = agent.history[-1]\n",
    "    writer.add_scalar(\"reward\",total_reward,episode)\n",
    "    writer.add_scalar(\"n_round\",n_round,episode)\n",
    "    writer.add_scalar(\"loss\",train_loss,episode)\n",
    "    \n",
    "    if total_reward>best_reward:\n",
    "        print(\"New record:\", total_reward)\n",
    "        best_reward=total_reward\n",
    "        writer.save(net, \"best.pth\")\n",
    "    \n",
    "    count_gamma = 0.5\n",
    "    running_reward = count_gamma*running_reward+(1-count_gamma)*total_reward\n",
    "    if (episode+1)%100==0:\n",
    "        print(episode, total_reward, running_reward)\n",
    "        writer.save(net, \"episode%s.pth\"%episode)\n",
    "    if running_reward>1:\n",
    "        break\n",
    "        \n",
    "writer.save(net, \"final.pth\")\n",
    "print(\"Finished: %s@%s\" %(agent.running_reward,episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.export_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
