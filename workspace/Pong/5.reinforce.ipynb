{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without global pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# from torchvision import transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-06 19:21:21,444] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downsample = 2\n",
    "output_size = 160//downsample\n",
    "\n",
    "def preprocess(frame):\n",
    "    '''from karpathy.'''\n",
    "    I = frame\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::downsample,::downsample,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    tensor = torch.from_numpy(I).float()\n",
    "    return tensor.unsqueeze(0).unsqueeze(0) #BCHW\n",
    "\n",
    "def clip_grads(net, low=-10, high=10):\n",
    "    \"\"\"Gradient clipping to the range [low, high].\"\"\"\n",
    "    parameters = [param for param in net.parameters()\n",
    "                  if param.grad is not None]\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(low, high)\n",
    "        \n",
    "if torch.cuda.is_available():\n",
    "    def to_var(x, requires_grad=False, gpu=None):\n",
    "        x = x.cuda(gpu)\n",
    "        return Variable(x, requires_grad=requires_grad)\n",
    "else:\n",
    "    def to_var(x, requires_grad=False, vgpu=None):\n",
    "        return Variable(x, requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''very similar to Nature DQN.'''\n",
    "    def __init__(self, action_n, input_shape=(1,80,80)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(input_shape[0],16,kernel_size=8, stride=2),nn.ReLU(),\n",
    "                                  nn.Conv2d(16,32,kernel_size=4, stride=2),nn.ReLU())\n",
    "        flatten_size = self._get_flatten_size(input_shape)\n",
    "        self.fc = nn.Linear(flatten_size, action_n)\n",
    "    \n",
    "    def _get_flatten_size(self, shape):\n",
    "        x = Variable(torch.rand(1, *shape))\n",
    "        output_feat = self.conv(x)\n",
    "        n_size = output_feat.view(-1).size(0)\n",
    "        return n_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)\n",
    "        logit = self.fc(feat.view(feat.size(0),-1))\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    \n",
    "    def __init__(self, model, gamma=0.99, eps=1.e-6, running_gamma=0.99, running_start=0,\n",
    "#                 episode2thresh=lambda i: 0.05+0.9*np.exp(-1. * i / 100) if i>150 else 0): # eploration will start after 150 episodes\n",
    "                 episode2thresh=lambda i: 0): # without exploration\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.total_rewards = []\n",
    "        self.running_reward = running_start\n",
    "        self.running_gamma = running_gamma\n",
    "        self.episode2thresh = episode2thresh\n",
    "        \n",
    "    @property\n",
    "    def episodes(self):\n",
    "        return len(self.total_rewards)\n",
    "        \n",
    "    def select_action(self,obs):\n",
    "        self.model.train()\n",
    "        thresh=self.episode2thresh(self.episodes)\n",
    "        action, log_prob = select_action(obs, self.model, thresh=thresh)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def get_loss_and_clear(self):\n",
    "        total_reward = sum(self.rewards)\n",
    "        self.total_rewards.append(total_reward)\n",
    "        self.running_reward = self.running_gamma*self.running_reward+(1-self.running_gamma)*total_reward\n",
    "        policy_loss = get_policy_loss(self.log_probs, self.rewards, self.gamma, self.eps)\n",
    "        del self.log_probs[:]\n",
    "        del self.rewards[:]\n",
    "        return policy_loss\n",
    "    \n",
    "    def take_action(self, action, env, render=False):\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        self.rewards.append(reward)\n",
    "        if render:\n",
    "            env.render()\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def greedy_policy(self, obs):\n",
    "        self.model.eval()\n",
    "        state = to_var(obs)\n",
    "        prob = self.model(state)\n",
    "        _, action = prob.max(dim=1)\n",
    "        return action.data[0]\n",
    "\n",
    "def select_action(obs, model, thresh=0):\n",
    "    state = to_var(obs)\n",
    "    logits = model(state)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "#     if np.random.random()>thresh:\n",
    "# #         print(probs)\n",
    "#         try:\n",
    "#             action = m.sample()\n",
    "#         except:\n",
    "#             print(probs,m)\n",
    "#             raise\n",
    "#     else:\n",
    "#         action_space = probs.size(1)\n",
    "#         action = to_var(torch.from_numpy(np.random.randint(action_space,size=1)))\n",
    "    return action.data[0],m.log_prob(action)\n",
    "    \n",
    "def get_normalized_rewards(rewards, gamma, eps):\n",
    "    acc = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        acc.append(R)\n",
    "    ret = to_var(torch.Tensor(acc[::-1]),requires_grad=False)\n",
    "    ret = (ret - ret.mean()) / (ret.std()+eps)\n",
    "#     print(ret)\n",
    "    return ret\n",
    "\n",
    "def get_policy_loss(log_probs,rewards, gamma,eps):\n",
    "    log_probs_v = torch.cat(log_probs)\n",
    "    rewards_v = get_normalized_rewards(rewards, gamma, eps)\n",
    "    return -log_probs_v.dot(rewards_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Net(env.action_space.n, input_shape=(1,output_size,output_size))\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1.e-4,weight_decay=0.001)\n",
    "trainer = PolicyGradient(model=net,running_start=-21)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "writer_path = list(writer.all_writers.keys())[0]\n",
    "weight_join = lambda p: os.path.join(writer_path, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/Jan06_19-21-25_amax'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:03<97:22:17,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -21.0 -21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 101/100000 [04:00<60:47:25,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 -21.0 -20.60003988479197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 201/100000 [08:04<66:25:24,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 -20.0 -20.31959998606043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 301/100000 [12:06<64:02:42,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 -21.0 -20.320732836342362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 401/100000 [16:09<72:04:21,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 -20.0 -20.342961946651958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/100000 [20:15<64:53:05,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 -21.0 -20.252606185330784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 601/100000 [24:17<68:55:34,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 -20.0 -20.22231845371272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 701/100000 [28:31<70:42:48,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 -21.0 -20.224567226904462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 801/100000 [33:01<68:32:56,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 -20.0 -19.963226569468443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 901/100000 [37:41<81:40:35,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 -19.0 -19.77328486511979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1001/100000 [42:22<74:42:56,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 -19.0 -19.562786137118174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1101/100000 [47:01<71:26:02,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100 -20.0 -19.57925452791731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1201/100000 [52:03<91:37:04,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 -20.0 -19.38045400428027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1301/100000 [57:33<89:03:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300 -20.0 -19.389766143118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1401/100000 [1:02:56<89:55:16,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 -20.0 -19.306894639312485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1501/100000 [1:08:49<108:38:48,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 -19.0 -19.501382920798143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1601/100000 [1:15:10<107:17:06,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 -19.0 -19.40060317051487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1701/100000 [1:21:47<113:59:46,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700 -21.0 -19.462713508964896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1801/100000 [1:28:56<119:01:27,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 -19.0 -19.1899354561199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1901/100000 [1:36:35<139:44:27,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900 -19.0 -19.085404361165168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2001/100000 [1:45:00<143:06:45,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 -19.0 -19.059987364707926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2101/100000 [1:53:52<145:29:30,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100 -21.0 -18.877655958861315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2201/100000 [2:03:29<184:42:07,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200 -15.0 -18.885703436316735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2301/100000 [2:13:36<187:04:32,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300 -13.0 -18.59343540495268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2339/100000 [2:17:49<180:29:23,  6.65s/it]"
     ]
    }
   ],
   "source": [
    "for episode in trange(100000):\n",
    "    frame = env.reset()\n",
    "    last_obs = preprocess(frame)\n",
    "    curr_obs = preprocess(frame)\n",
    "    total_reward = 0\n",
    "    for step in range(100000): # not exceed 10000 steps\n",
    "        action = trainer.select_action(obs=curr_obs-last_obs)\n",
    "        frame, reward, done, _ = trainer.take_action(action, env, render=False)\n",
    "        last_obs = curr_obs\n",
    "        curr_obs = preprocess(frame)\n",
    "        total_reward+=reward\n",
    "        if done:\n",
    "             break\n",
    "    if step==100000:\n",
    "        print(\"not enough!!!!!!!!!!!!!!!\")\n",
    "    policy_loss = trainer.get_loss_and_clear()\n",
    "    writer.add_scalar(\"loss\",policy_loss.data[0],episode)\n",
    "    writer.add_scalar(\"reward\",total_reward,episode)\n",
    "#     print(policy_loss)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    clip_grads(trainer.model,-5,5)\n",
    "    optimizer.step()\n",
    "    running_reward = trainer.running_reward\n",
    "    if episode%100==0:\n",
    "        print(episode, total_reward,running_reward)\n",
    "        torch.save(net.state_dict(), weight_join(\"episode%s.pth\"%episode))\n",
    "    if running_reward>1:\n",
    "        break\n",
    "print(\"Finished: %s@%s\" %(trainer.running_reward,episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), weight_join(\"final.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(trainer.total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
